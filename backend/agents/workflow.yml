# 1. LLM DEFINITIONS (THE "BRAINS")
# We're defining our two NIM endpoints.
# The toolkit will automatically load the NVIDIA_API_KEY from the .env file.
llms:
  high_cost_nim:
    _type: nim
    model_name: "nvidia/llama-3.3-nemotron-super-49b-v1.5"
    base_url: "https://integrate.api.nvidia.com/v1/"
  
  low_cost_nim:
    _type: nim
    model_name: "nvidia/nvidia-nemotron-nano-9b-v2"
    base_url: "https://integrate.api.nvidia.com/v1/"

# 2. FUNCTION DEFINITIONS (THE "TOOLS")
# These are our four complex, Python-based pipeline agents.
# We are just registering their callable function names here.
functions:
  - _type: python
    callable: labeling_pipeline.run_labeling_pipeline
    description: >-
      (Pipeline 1: Label) Runs the full self-correcting weak supervision pipeline.
      Takes a user goal (e.g., 'label for spam') and raw data path.
      Returns the path to the final, labeled parquet file.

  - _type: python
    callable: training_pipeline.run_training_pipeline
    description: >-
      (Pipeline 2: Train) Runs the full self-correcting training and tuning pipeline.
      Takes a labeled data path and a holdout test set path.
      Trains a generative model (e.g., cVAE) and returns paths to the final config, model, and preprocessor.

  - _type: python
    callable: anomaly_pipeline.run_anomaly_pipeline
    description: >-
      (Pipeline 3: Clean) Scans a dataset with a trained model to find anomalies.
      Takes the model paths and a data path to scan.
      Returns the path to an anomaly report.

  - _type: python
    callable: generation_pipeline.run_generation_pipeline
    description: >-
      (Pipeline 4: Generate) Generates new synthetic data from a trained model.
      Takes the model paths, a desired label, number of samples, and output format.
      Returns the path to the synthetic data file.

# 3. WORKFLOW DEFINITION (THE "ORCHESTRATOR AGENT")
# This is our main agent. It's a ReAct agent that uses the
# high-cost brain and has access to all four pipeline tools.
workflow:
  _type: react_agent
  llm: high_cost_nim
  system_prompt: |
    You are a master Orchestrator for the DataFoundry platform.
    Your job is to take a high-level, natural-language user goal and break it down
    into a step-by-step execution plan using your available tools.

    You MUST carefully manage the file paths and dependencies between steps.
    For example, 'run_training_pipeline' MUST be called after 'run_labeling_pipeline'
    and its output ('labeled_data_path') must be passed as an input.

    Plan your steps, execute them one by one, and confirm the final output for the user.
  tools:
    - run_labeling_pipeline
    - run_training_pipeline
    - run_anomaly_pipeline
    - run_generation_pipeline